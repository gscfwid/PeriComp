{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No think label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most models can use the schema below.\n",
    "no_think_schema = {\n",
    "    \"name\": \"no_think_label\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"description\": \"post-operative complications that arose during the medical case.\",\n",
    "        \"properties\": {\n",
    "            \"complications\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"List of post-operative complications that arose during the medical case.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Name of the complication.\"\n",
    "                        },\n",
    "                        \"grading\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Severity grading of the complication.\",\n",
    "                            \"enum\": [\n",
    "                                \"轻\",\n",
    "                                \"中\",\n",
    "                                \"重\",\n",
    "                                \"Null\"\n",
    "                            ]\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"name\",\n",
    "                        \"grading\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# claude use tool_use schema like below\n",
    "no_think_tools = [\n",
    "    {\n",
    "        \"name\": \"medical_case_analysis\",\n",
    "        \"description\": \"Analyze post-operative complications of medical case and provide structured analysis\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"post-operative complications that arose during the medical case.\",\n",
    "            \"properties\": {\n",
    "                \"complications\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"List of post-operative complications that arose during the medical case.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Name of the complication.\"\n",
    "                            },\n",
    "                            \"grading\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Severity grading of the complication.\",\n",
    "                                \"enum\": [\n",
    "                                    \"轻\",\n",
    "                                    \"中\",\n",
    "                                    \"重\",\n",
    "                                    \"Null\"\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"name\",\n",
    "                            \"grading\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"complications\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having think label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most models can use the schema below.\n",
    "think_label_schema = {\n",
    "    \"name\": \"having_think_label\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"think\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"A sequence of thoughts or considerations regarding the medical case.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"complications\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"List of post-operative complications that arose during the medical case.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Name of the complication.\"\n",
    "                        },\n",
    "                        \"grading\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Severity grading of the complication.\",\n",
    "                            \"enum\": [\n",
    "                                \"轻\",\n",
    "                                \"中\",\n",
    "                                \"重\",\n",
    "                                \"Null\"\n",
    "                            ]\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"name\",\n",
    "                        \"grading\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"think\",\n",
    "            \"complications\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Claude tool_use schema (new format)\n",
    "think_label_tools = [\n",
    "    {\n",
    "        \"name\": \"medical_case_analysis\",\n",
    "        \"description\": \"Analyze medical case complications and provide structured analysis\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"think\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"A sequence of thoughts or considerations regarding the medical case.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"complications\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"List of post-operative complications that arose during the medical case.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Name of the complication.\"\n",
    "                            },\n",
    "                            \"grading\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Severity grading of the complication.\",\n",
    "                                \"enum\": [\n",
    "                                    \"轻\",\n",
    "                                    \"中\",\n",
    "                                    \"重\",\n",
    "                                    \"Null\"\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"name\",\n",
    "                            \"grading\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"think\",\n",
    "                \"complications\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most models can use the schema below.\n",
    "targeted_schema = {\n",
    "    \"name\": \"targeted_schema\",\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"think\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Thoughts or considerations regarding the complication.\",\n",
    "            },\n",
    "            \"bool\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the complication is diagnosed.\"\n",
    "            },\n",
    "            \"grading\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Severity grading of the complication.\",\n",
    "                \"enum\": [\n",
    "                    \"轻\",\n",
    "                    \"中\",\n",
    "                    \"重\",\n",
    "                    \"Null\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"think\",\n",
    "            \"bool\",\n",
    "            \"grading\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define request funciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai compatible api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes GPT series, DeepSeek series, Grok series, Qwen series, etc.\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "api_key = \"<your_api_key>\"\n",
    "api_base = \"<your_api_base>\"\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "timeout = 2000\n",
    "sleep_time = 5\n",
    "# response_format = \"no_think_schema\" # can be \"no_think_schema\", \"think_label_schema\", \"targeted_schema\"\n",
    "\n",
    "async def fetch_openai_response(prompt, model:str, client=client, response_format=None):\n",
    "    while True:\n",
    "        try:\n",
    "            completion_args = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_tokens\": 2000,\n",
    "                \"timeout\": timeout\n",
    "            }           \n",
    "            if response_format: # some model may not support json schema, this parameter should be modified according to the model\n",
    "                completion_args[\"response_format\"] = {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": response_format\n",
    "                }\n",
    "            print(\"开始请求...\")\n",
    "            response = await client.chat.completions.create(**completion_args)\n",
    "            if response and response.choices:\n",
    "                print(\"获取结果成功\")\n",
    "                # Retrieve reasoning content and final response\n",
    "                if \"openrouter\" in api_base:\n",
    "                    reasoning_content = getattr(response.choices[0].message, 'reasoning', \"\") # 获取思维链\n",
    "                else:\n",
    "                    reasoning_content = getattr(response.choices[0].message, 'reasoning_content', \"\") # 获取思维链\n",
    "                content = response.choices[0].message.content\n",
    "                # return a dictionary containing reasoning content and final response\n",
    "                if content:\n",
    "                    return {\n",
    "                        \"reasoning_content\": reasoning_content,\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"No response received from OpenAI, retrying...\")\n",
    "                await asyncio.sleep(sleep_time)\n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"Request timed out after {timeout} seconds, retrying...\")\n",
    "            await asyncio.sleep(sleep_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}, retrying...\")\n",
    "            await asyncio.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "api_key = \"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "async def fetch_gemini_response(prompt, model_name, response_format):    \n",
    "    while True:\n",
    "        try:\n",
    "            client = genai.Client(\n",
    "                api_key=api_key,\n",
    "            )\n",
    "\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[\n",
    "                        types.Part.from_text(text=prompt), \n",
    "                   ],\n",
    "                ),\n",
    "            ]\n",
    "            if response_format:\n",
    "                config = {\n",
    "                    'response_mime_type': 'application/json',\n",
    "                    'response_schema': response_format[\"schema\"]\n",
    "                }\n",
    "            else:\n",
    "                config = {}\n",
    "\n",
    "            response = await asyncio.to_thread(\n",
    "                client.models.generate_content,\n",
    "                model=model_name,\n",
    "                contents=contents,\n",
    "                config=config\n",
    "            )\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Google API error: {e}, retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude API request function\n",
    "import anthropic\n",
    "import time\n",
    "\n",
    "api_key = \"YOUR_ANTHROPIC_API_KEY_HERE\"\n",
    "\n",
    "def fetch_claude_response(prompt, model, think=True, tool_use=None):\n",
    "    while True:\n",
    "        try:\n",
    "            claude_client = anthropic.Anthropic(api_key=api_key)\n",
    "            completion_args = {\n",
    "                \"model\": model,\n",
    "                \"max_tokens\": 2000,\n",
    "                \"system\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt[\"system\"], \"cache_control\": {\"type\": \"ephemeral\"}}, # use cache to reduce token usage\n",
    "                ],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt[\"user\"]}\n",
    "                ]\n",
    "            }\n",
    "            if think:\n",
    "                completion_args[\"thinking\"] = {\n",
    "                    \"type\": \"enabled\",\n",
    "                    \"budget_tokens\": 2000,\n",
    "                }\n",
    "            if tool_use:\n",
    "                completion_args[\"tools\"] = tool_use\n",
    "                \n",
    "            response = claude_client.messages.create(**completion_args)\n",
    "\n",
    "            if not response:\n",
    "                raise ValueError(\"No valid response received from Claude API.\")\n",
    "\n",
    "            print(\"Successfully retrieved Claude result\")\n",
    "            print(f\"Cache usage: {response.usage}\")\n",
    "            \n",
    "            # Process response content\n",
    "            has_thinking = any(\n",
    "                block.type == \"thinking\" for block in response.content\n",
    "            )\n",
    "            if has_thinking:\n",
    "                print(\"thinking\")\n",
    "                reasoning_content = \"\\n\".join(block.thinking for block in response.content if block.type == \"thinking\")\n",
    "                print(reasoning_content)\n",
    "            else:\n",
    "                reasoning_content = \"\"\n",
    "            \n",
    "            # process tool use input\n",
    "            tool_input = None\n",
    "            # find tool_use block in content\n",
    "            for block in response.content:\n",
    "                if block.type == \"tool_use\":\n",
    "                    tool_input = block.input\n",
    "                    break\n",
    "\n",
    "            if tool_input:\n",
    "                content = tool_input\n",
    "            elif hasattr(response, 'content') and response.content:\n",
    "                # backup plan: if no tool_use but has content\n",
    "                content = response.content[0].text if hasattr(response.content[0], 'text') else \"{}\"\n",
    "            else:\n",
    "                content = \"{}\"\n",
    "            \n",
    "            return {\n",
    "                \"reasoning_content\": reasoning_content,\n",
    "                \"content\": content\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Claude API error: {e}, retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# read prompts\n",
    "\n",
    "import json\n",
    "with open(\"prompt/comprehensive_prompts_nothink.json\", \"r\") as f:\n",
    "    comprehensive_prompts_nothink = json.load(f)\n",
    "\n",
    "with open(\"prompt/comprehensive_prompts.json\", \"r\") as f:\n",
    "    comprehensive_prompts = json.load(f)\n",
    "\n",
    "with open(\"prompt/comprehensive_prompts_modified.json\", \"r\") as f:\n",
    "    comprehensive_prompts_modified = json.load(f)\n",
    "\n",
    "print(len(comprehensive_prompts_nothink))\n",
    "print(len(comprehensive_prompts))\n",
    "print(len(comprehensive_prompts_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openai and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始请求...\n",
      "获取结果成功\n",
      "Patient 1 获取成功，已完成1/1个请求\n"
     ]
    }
   ],
   "source": [
    "# This section initiates requests to the large language model.\n",
    "# We will manually define the calling function, model, output directory, and other parameters for each request.\n",
    "# Below are three illustrative examples.\n",
    "# First is openai compatible models (gpt series, deepseek series, etc.)\n",
    "output = \"results/raw/o1_comprehensive_nothink.json\"\n",
    "model = \"openai/o1\" # can be other openai compatible models\n",
    "prompt = comprehensive_prompts_nothink # can be comprehensive_prompts, comprehensive_prompts_modified\n",
    "\n",
    "response_format = no_think_schema # can be think_label_schema, targeted_schema.\n",
    "\n",
    "start = 0 # start from the first prompt\n",
    "end = len(comprehensive_prompts_nothink) # end at the last prompt\n",
    "batch_size = 5 # enable batch processing\n",
    "sleep_time = 5 # sleep time between requests\n",
    "\n",
    "# batch processing\n",
    "for i in range(start, end, batch_size):\n",
    "    batch_prompts = list(comprehensive_prompts_nothink.values())[i:i + batch_size]\n",
    "    batch_pt_nos = list(comprehensive_prompts_nothink.keys())[i:i + batch_size]\n",
    "    batch_tasks = [fetch_openai_response(prompt, model, response_format) for prompt in batch_prompts]\n",
    "    \n",
    "    batch_responses = await asyncio.gather(*batch_tasks)\n",
    "    \n",
    "    if \"final_dict\" not in globals():\n",
    "        final_dict = {}\n",
    "    \n",
    "    for pt_no, response in zip(batch_pt_nos, batch_responses):\n",
    "        final_dict[pt_no] = response\n",
    "        print(f\"Patient {pt_no} retrieved successfully, {i+1}/{end} requests completed\")\n",
    "    \n",
    "    await asyncio.sleep(sleep_time)\n",
    "    \n",
    "    with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient 1 获取成功，已完成1/1个请求\n"
     ]
    }
   ],
   "source": [
    "# Second is Gemini series models\n",
    "output = \"results/raw/gemini2.5_flash_comprehensive_nothink.json\"\n",
    "model = \"gemini-2.5-flash\" # can be gemini pro\n",
    "prompt = comprehensive_prompts_nothink # can be comprehensive_prompts, comprehensive_prompts_modified\n",
    "\n",
    "response_format = no_think_schema # can be think_label_schema, targeted_schema.\n",
    "\n",
    "start = 0 # start from the first prompt\n",
    "end = len(comprehensive_prompts_nothink) # end at the last prompt\n",
    "batch_size = 5 # enable batch processing\n",
    "sleep_time = 5 # sleep time between requests\n",
    "\n",
    "# batch processing\n",
    "for i in range(start, end, batch_size):\n",
    "    batch_prompts = list(comprehensive_prompts_nothink.values())[i:i + batch_size]\n",
    "    batch_pt_nos = list(comprehensive_prompts_nothink.keys())[i:i + batch_size]\n",
    "    batch_tasks = [fetch_gemini_response(prompt, model, response_format) for prompt in batch_prompts] # can be fetch_gemini_response\n",
    "    \n",
    "    batch_responses = await asyncio.gather(*batch_tasks)\n",
    "    \n",
    "    if \"final_dict\" not in globals():\n",
    "        final_dict = {}\n",
    "    \n",
    "    for pt_no, response in zip(batch_pt_nos, batch_responses):\n",
    "        final_dict[pt_no] = response\n",
    "        print(f\"Patient {pt_no} retrieved successfully, {i+1}/{end} requests completed\")\n",
    "    \n",
    "    await asyncio.sleep(sleep_time)\n",
    "    \n",
    "    with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved Claude result\n",
      "Cache usage: Usage(cache_creation_input_tokens=3628, cache_read_input_tokens=0, input_tokens=8222, output_tokens=319, service_tier='standard')\n",
      "Processing 1/1\n"
     ]
    }
   ],
   "source": [
    "# We need to separate the prompts to leverage caching and reduce token consumption.\n",
    "output = \"results/raw/claude3_7_sonnet_comprehensive_nothink.json\"\n",
    "model = \"claude-3-7-sonnet-20250219\"\n",
    "prompt = comprehensive_prompts_nothink\n",
    "\n",
    "for i, (k, v) in enumerate(prompt.items()):\n",
    "    pt_no = k\n",
    "    prompt_combined = v.split(\"### 病历资料\")\n",
    "    response = fetch_claude_response({\"system\": prompt_combined[0], \"user\": \"### 病历资料\\n\" + prompt_combined[1]}, model=model, think=False, tool_use=no_think_tools)\n",
    "    final_dict[pt_no] = response\n",
    "    print(f\"Processing {i+1}/{len(comprehensive_prompts_nothink)}\")\n",
    "    with open(output, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(final_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"prompt/targeted_prompts.json\", \"r\") as f:\n",
    "    prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process 1/1\n",
      "Now processing records 0 to 1\n",
      "开始请求急性肾损伤...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求急性呼吸窘迫综合征 (ARDS)...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求吻合口破裂...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求心律失常...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求心脏骤停...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求心源性肺水肿...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求深静脉血栓...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求谵妄...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求胃肠道出血...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求感染（来源不明）...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求血流感染（实验室确诊）...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求心肌梗死...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求非心脏手术后心肌损伤...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求肺炎...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求麻痹性肠梗阻...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求术后出血...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求肺栓塞...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求中风...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求浅表手术部位感染...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求深部手术部位感染...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求器官/腔隙手术部位感染...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "开始请求泌尿道感染...\n",
      "开始请求...\n",
      "获取结果成功\n",
      "Processing completed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import aiohttp\n",
    "\n",
    "epoches = 1\n",
    "batch_size = 1\n",
    "start = 0\n",
    "end = len(prompts)\n",
    "\n",
    "response_format = None # if deepseek, use None because it doesn't support response_format\n",
    "# Reset API base and key for DeepSeek\n",
    "api_base = \"https://api.deepseek.com/v1\"\n",
    "api_key = \"YOUR_DEEPSEEK_API_KEY_HERE\"\n",
    "output = \"results/raw/deepseek_r1_targeted.json\"\n",
    "model = \"deepseek-reasoner\" # can be other openai compatible models\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "async def single_pt_response(prompts:dict):\n",
    "    result_dict = {}\n",
    "    for k, v in prompts.items():\n",
    "        print(f\"Processing {k}...\")\n",
    "        response = await fetch_openai_response(v, model = model, client=client, response_format=response_format)\n",
    "        result_dict[k] = response\n",
    "    return result_dict\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "\n",
    "    for epoch in range(1, epoches+1):\n",
    "        print(f\"Starting to process {epoch}/{epoches}\")\n",
    "        final_dict = {}\n",
    "        # Process requests in batches\n",
    "        for i in range(start, end, batch_size):\n",
    "            batch_prompts = list(prompts.values())[i:i + batch_size]\n",
    "            batch_pt_nos = list(prompts.keys())[i:i + batch_size]\n",
    "            \n",
    "            # Create tasks for current batch\n",
    "            batch_tasks = [single_pt_response(prompt) for prompt in batch_prompts]\n",
    "            print(f\"Now processing records {i} to {i+batch_size}\")\n",
    "            \n",
    "            # Wait for current batch to complete\n",
    "            batch_responses = await asyncio.gather(*batch_tasks)\n",
    "            print(\"Processing completed\")\n",
    "           \n",
    "            # Save results\n",
    "            for pt_no, response in zip(batch_pt_nos, batch_responses):\n",
    "                final_dict[pt_no] = response\n",
    "            \n",
    "            # Pause after each batch to avoid too frequent requests\n",
    "            await asyncio.sleep(1)\n",
    "    \n",
    "        # Save all results to file after each batch\n",
    "        with open(output, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(final_dict, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
